# 双解码器回归网络架构与训练流程图

## 1. 训练流程图

### 1.1 整体训练流程

```mermaid
flowchart TD
    A[开始] --> B[准备数据集]
    B --> C[阶段1: 全监督预训练]
    C --> D[nnU-Net v2训练]
    D --> E[教师模型权重]
    
    E --> F[阶段2: 半监督训练]
    F --> G[师生框架设置]
    G --> H[强弱一致性训练]
    H --> I[无标签数据训练]
    I --> J[学生模型权重]
    
    J --> K[阶段3: 双解码器回归训练]
    K --> L[加载学生模型权重]
    L --> M[初始化双解码器网络]
    M --> N[冻结分割解码器]
    N --> O[训练回归解码器]
    O --> P[验证性能]
    P --> Q{收敛?}
    
    Q -->|否| R[调整超参数]
    R --> O
    
    Q -->|是| S[保存最终模型]
    S --> T[结束]
    
    style C fill:#e1f5fe
    style F fill:#f3e5f5
    style K fill:#fff3e0
    style N fill:#ffebee
```

### 1.2 详细三阶段训练流程

```mermaid
flowchart LR
    subgraph "阶段1: 全监督预训练"
        A1[标注分割数据] --> B1[nnU-Net v2训练]
        B1 --> C1[教师模型权重]
    end
    
    subgraph "阶段2: 半监督训练"
        A2[无标签数据] --> B2[师生框架]
        C1 --> B2
        B2 --> C2[强弱一致性损失]
        C2 --> D2[学生模型训练]
        D2 --> E2[学生模型权重]
    end
    
    subgraph "阶段3: 双解码器训练"
        A3[回归标签数据] --> B3[权重初始化]
        E2 --> B3
        B3 --> C3[冻结分割解码器]
        C3 --> D3[训练回归解码器]
        D3 --> E3[最终双解码器模型]
    end
    
    C1 --> B2
    E2 --> B3
    
    style A1 fill:#e8f5e8
    style A2 fill:#fff2e8
    style A3 fill:#ffe8e8
    style E3 fill:#f3e5f5
```

## 2. 双解码器网络结构图

### 2.1 整体网络架构

```mermaid
graph TD
    subgraph "输入层"
        Input["医学图像<br/>H×W×D×C"]
    end
    
    subgraph "共享编码器"
        E1[编码层1<br/>32通道]
        E2[编码层2<br/>64通道]
        E3[编码层3<br/>128通道]
        E4[编码层4<br/>256通道]
        E5[编码层5<br/>320通道]
        E6[编码层6<br/>320通道]
    end
    
    subgraph "分割解码器 (冻结)"
        SD1[分割解码1<br/>256通道]
        SD2[分割解码2<br/>128通道]
        SD3[分割解码3<br/>64通道]
        SD4[分割解码4<br/>32通道]
        SD5[分割输出<br/>类别数]
    end
    
    subgraph "回归解码器 (可训练)"
        RD1[回归解码1<br/>256通道]
        RD2[回归解码2<br/>128通道]
        RD3[回归解码3<br/>64通道]
        RD4[全局池化<br/>1024维]
        RD5[全连接层<br/>512维]
        RD6[回归输出<br/>1维]
    end
    
    subgraph "交叉注意力模块"
        CA[注意力计算<br/>Q, K, V]
    end
    
    Input --> E1
    E1 --> E2
    E2 --> E3
    E3 --> E4
    E4 --> E5
    E5 --> E6
    
    E6 --> SD1
    E5 --> SD1
    SD1 --> SD2
    E4 --> SD2
    SD2 --> SD3
    E3 --> SD3
    SD3 --> SD4
    E2 --> SD4
    SD4 --> SD5
    
    E6 --> RD1
    E5 --> RD1
    RD1 --> RD2
    E4 --> RD2
    RD2 --> RD3
    E3 --> RD3
    RD3 --> RD4
    RD4 --> RD5
    RD5 --> RD6
    
    SD3 --> CA
    RD3 --> CA
    CA --> RD4
    
    style E1 fill:#e3f2fd
    style E2 fill:#e3f2fd
    style E3 fill:#e3f2fd
    style E4 fill:#e3f2fd
    style E5 fill:#e3f2fd
    style E6 fill:#e3f2fd
    style SD1 fill:#e8f5e8
    style SD2 fill:#e8f5e8
    style SD3 fill:#e8f5e8
    style SD4 fill:#e8f5e8
    style SD5 fill:#e8f5e8
    style RD1 fill:#fff3e0
    style RD2 fill:#fff3e0
    style RD3 fill:#fff3e0
    style RD4 fill:#fff3e0
    style RD5 fill:#fff3e0
    style RD6 fill:#fff3e0
    style CA fill:#f3e5f5
```

### 2.2 交叉注意力机制详细结构

```mermaid
graph LR
    subgraph "分割特征"
        SF["分割特征<br/>H×W×D×C"]
    end
    
    subgraph "回归特征"
        RF["回归特征<br/>H×W×D×C"]
    end
    
    subgraph "注意力计算"
        Q[Query<br/>回归特征]
        K[Key<br/>分割特征]
        V[Value<br/>分割特征]
        ATT["注意力权重<br/>Softmax(Q*K_T/sqrt(d))"]
        OUT[增强特征<br/>Attention(Q,K,V)]
    end
    
    subgraph "特征融合"
        ADD["残差连接<br/>RF + a×OUT"]
        FINAL[增强回归特征]
    end
    
    SF --> K
    SF --> V
    RF --> Q
    
    Q --> ATT
    K --> ATT
    ATT --> OUT
    V --> OUT
    
    OUT --> ADD
    RF --> ADD
    ADD --> FINAL
    
    style SF fill:#e8f5e8
    style RF fill:#fff3e0
    style ATT fill:#f3e5f5
    style FINAL fill:#ffebee
```

## 3. 损失函数计算流程

```mermaid
flowchart TD
    subgraph "前向传播"
        A[输入图像] --> B[共享编码器]
        B --> C[分割解码器]
        B --> D[回归解码器]
        C --> E[分割预测]
        D --> F[回归预测]
    end
    
    subgraph "损失计算"
        E --> G[分割损失<br/>Dice + CE]
        F --> H[回归损失<br/>MSE/MAE]
        G --> I["加权总损失<br/>a×L_seg + b×L_reg"]
        H --> I
    end
    
    subgraph "反向传播"
        I --> J{分割解码器冻结?}
        J -->|是| K[仅更新回归部分]
        J -->|否| L[更新所有参数]
        K --> M[参数更新]
        L --> M
    end
    
    style G fill:#e8f5e8
    style H fill:#fff3e0
    style I fill:#f3e5f5
    style K fill:#ffebee
```

## 4. 数据流向图

```mermaid
flowchart LR
    Input["输入数据<br/>100%"] --> Encoder["共享编码器"]
    Encoder --> SegDecoder["分割解码器<br/>50%"]
    Encoder --> RegDecoder["回归解码器<br/>50%"]
    SegDecoder --> SegOutput["分割输出<br/>45%"]
    SegDecoder --> CrossAttention["交叉注意力<br/>5%"]
    RegDecoder --> CrossAttention2["交叉注意力<br/>10%"]
    CrossAttention --> RegDecoder2["回归增强<br/>15%"]
    RegDecoder2 --> RegOutput["回归输出<br/>55%"]
    
    style Input fill:#e3f2fd
    style Encoder fill:#e3f2fd
    style SegDecoder fill:#e8f5e8
    style RegDecoder fill:#fff3e0
    style CrossAttention fill:#f3e5f5
    style CrossAttention2 fill:#f3e5f5
    style RegDecoder2 fill:#fff3e0
```

## 5. 权重传递流程图

```mermaid
flowchart LR
    subgraph "阶段1: 全监督预训练"
        A1[标注数据] --> A2[nnU-Net v2]
        A2 --> A3[教师模型权重]
    end
    
    subgraph "阶段2: 半监督训练"
        B1[无标签数据] --> B2[师生框架]
        A3 --> B2
        B2 --> B3[学生模型权重]
    end
    
    subgraph "阶段3: 双解码器初始化"
        C1[回归数据] --> C2[双解码器网络]
        B3 --> C3[编码器权重]
        B3 --> C4[分割解码器权重]
        C3 --> C2
        C4 --> C2
        C2 --> C5[冻结分割解码器]
        C2 --> C6[训练回归解码器]
    end
    
    style A3 fill:#e8f5e8
    style B3 fill:#f3e5f5
    style C5 fill:#c8e6c9
    style C6 fill:#fff3e0
```

## 6. 模型架构对比

```mermaid
graph TB
    subgraph "方法A: 标准nnU-Net v2"
        A1[输入] --> A2[编码器]
        A2 --> A3[分割解码器]
        A3 --> A4[分割输出]
    end
    
    subgraph "方法B: 我们的方法"
        B1[输入] --> B2[半监督训练的编码器]
        B2 --> B3[冻结分割解码器]
        B2 --> B4[可训练回归解码器]
        B3 --> B5[分割输出]
        B3 --> B6[交叉注意力]
        B4 --> B6
        B6 --> B4
        B4 --> B7[回归输出]
    end
    
    style A2 fill:#e3f2fd
    style A3 fill:#e8f5e8
    style B2 fill:#e1f5fe
    style B3 fill:#c8e6c9
    style B4 fill:#fff3e0
    style B6 fill:#f3e5f5
```

## 6. 半监督师生框架详细流程

```mermaid
flowchart TD
    subgraph "教师模型 (冻结)"
        T1[全监督训练的nnU-Net]
        T2[教师模型推理]
        T3[生成伪标签]
    end
    
    subgraph "学生模型 (可训练)"
        S1[学生网络初始化]
        S2[强数据增强]
        S3[弱数据增强]
        S4[学生模型推理]
        S5[一致性损失计算]
    end
    
    subgraph "数据流"
        D1[无标签数据]
        D2[强增强数据]
        D3[弱增强数据]
    end
    
    D1 --> D2
    D1 --> D3
    D3 --> T2
    T2 --> T3
    D2 --> S4
    T3 --> S5
    S4 --> S5
    S5 --> S1
    
    style T1 fill:#c8e6c9
    style S1 fill:#fff3e0
    style S5 fill:#f3e5f5
```

### 6.1 强弱一致性损失计算

```mermaid
flowchart LR
    subgraph "输入数据"
        A[无标签图像]
    end
    
    subgraph "数据增强"
        B[弱增强<br/>轻微变换]
        C[强增强<br/>重度变换]
    end
    
    subgraph "模型推理"
        D[教师模型<br/>冻结权重]
        E[学生模型<br/>可训练]
    end
    
    subgraph "损失计算"
        F[伪标签<br/>教师输出]
        G[学生预测<br/>强增强输入]
        H[一致性损失<br/>MSE/CE Loss]
    end
    
    A --> B
    A --> C
    B --> D
    C --> E
    D --> F
    E --> G
    F --> H
    G --> H
    
    style B fill:#e8f5e8
    style C fill:#ffebee
    style D fill:#c8e6c9
    style E fill:#fff3e0
    style H fill:#f3e5f5
```

## 7. 训练策略对比

```mermaid
timeline
    title 训练策略时间线对比
    
    section 标准方法
        分割训练    : 24小时
        回归训练    : 24小时
        总计       : 48小时
    
    section 我们的方法
        全监督预训练 : 24小时
        半监督训练   : 12小时
        双解码器训练 : 4小时
        总计       : 40小时
```

---

**说明**：
- 蓝色：共享编码器组件
- 绿色：分割相关组件
- 橙色：回归相关组件  
- 紫色：注意力机制
- 红色：关键决策点

这些图表清晰展示了双解码器回归网络的完整架构和训练流程，便于理解和汇报使用。